{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ba5f01-252d-4e17-981f-14052822fc91",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "Here we only define the paths that we are going to use throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f511f4-ff3c-4054-8fc3-41231e1e844d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T13:00:00.851852790Z",
     "start_time": "2024-01-19T13:00:00.820307968Z"
    }
   },
   "outputs": [],
   "source": [
    "path_split = \"/data/kaiclasen/split.csv\"\n",
    "path_labels = \"/data/kaiclasen/lbls.parquet\"\n",
    "path_image_lmdb = \"/data/kaiclasen/BENv2.lmdb\"\n",
    "path_s2v2name_to_s1v1name = \"/data/kaiclasen/new_s2s1_mapping.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc846e3a-d5e3-4301-9e4f-cff8769160b5",
   "metadata": {},
   "source": [
    "# Read patch names\n",
    "\n",
    "For the dataset that we are using later, we can eighter say we want to use all data in the data base or we choose to supply a list of patch names that we want to use.\n",
    "To supply the _official split_, we simply read the `split.csv` and filter then by the `split` column into the three different predefined splits.\n",
    "However, right now this `.csv` contains patches that are filtered during creation of the data set, which means that not all names contained here actually exist within the LMDB file or have labels.\n",
    "Therefore we filter all the patches that don't have any labels associated with them.\n",
    "For easier access later we save this information in a `dict`.\n",
    "\n",
    "Of cause generating this list is later up to the user of the DS and can be substituded for any other selection method like season- or country-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c8ca74-e66e-44ac-891f-481a555f06bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T13:00:04.312469889Z",
     "start_time": "2024-01-19T13:00:01.985429642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     name       split\n",
      "0       S2B_MSIL2A_20171206T094349_N9999_R036_T34TCR_8...       train\n",
      "1       S2A_MSIL2A_20171002T094031_N9999_R036_T34TCR_8...       train\n",
      "2       S2A_MSIL2A_20170613T101031_N9999_R022_T34VER_8...       train\n",
      "3       S2B_MSIL2A_20180525T094029_N9999_R036_T35VNK_4...  validation\n",
      "4       S2B_MSIL2A_20180522T093029_N9999_R136_T35VPJ_3...        test\n",
      "...                                                   ...         ...\n",
      "549483  S2A_MSIL2A_20180413T095031_N9999_R079_T35VLG_6...        test\n",
      "549484  S2B_MSIL2A_20180224T112109_N9999_R037_T29SNC_0...       train\n",
      "549485  S2B_MSIL2A_20180223T101019_N9999_R022_T34WFT_7...  validation\n",
      "549486  S2B_MSIL2A_20170817T101019_N9999_R022_T34WFS_4...        test\n",
      "549487  S2B_MSIL2A_20180326T112109_N9999_R037_T29SNB_4...        test\n",
      "\n",
      "[549488 rows x 2 columns]\n",
      "{'test': ['S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_26_57',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_55',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_56',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_57',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_58',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_59',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_60',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_27_61',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_28_55',\n",
      "          'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_28_56'],\n",
      " 'train': ['S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_37_88',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_37_89',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_37_90',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_38_87',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_38_88',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_38_89',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_38_90',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_39_86',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_39_87',\n",
      "           'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_39_88'],\n",
      " 'validation': ['S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_33_69',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_33_70',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_34_69',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_34_70',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_34_71',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_35_69',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_35_70',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_35_71',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_36_69',\n",
      "                'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_36_70']}\n",
      "\n",
      "{'test': 137367, 'train': 272544, 'validation': 139577}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# read the csv for split info\n",
    "df = pd.read_csv(path_split)\n",
    "\n",
    "# get all patches without dublicates \n",
    "# -> this means not all labels are in this table but we don't use it for label information at this point anyways\n",
    "lbls = pd.read_parquet(path_labels).drop_duplicates(['patch'])\n",
    "\n",
    "# get only patches that also have a label, drop the additional columns\n",
    "df = df.merge(lbls, how='inner', left_on=['name'], right_on=['patch']).drop(['lbl_19', 'patch'], axis=1)\n",
    "print(df, end='\\n\\n')\n",
    "\n",
    "# filter by split column and write into dict\n",
    "patches = {\n",
    "    split: sorted(list(df[df.split == split].name.values)) for split in ['train', 'validation', 'test']\n",
    "}\n",
    "\n",
    "# show first 10 entries in each split\n",
    "# due to the naming scema and the frame-like splitting, we can nicely see the differences for the split \n",
    "#    for the patches\n",
    "pprint({s: patches[s][:10] for s in patches.keys()})\n",
    "print()\n",
    "pprint({s: len(patches[s]) for s in patches.keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff51b33-dbda-4d57-b6c5-2689b0767e7b",
   "metadata": {},
   "source": [
    "# Creating the DataSet\n",
    "\n",
    "To create the dataset for the respective splits, we can just use the DataSet class provided and later pass it to the DataLoader.\n",
    "We have to supply the files for the image data `image_lmdb_file`, the file containing the labels `label_file` and the mapping for lookup from the _NEW_ S2 name to the _OLD_ S1 name `s2s1_mapping_file`. \n",
    "This mapping is not required if you only want to use the S2 images.\n",
    "\n",
    "Optionally we can also supply the band names that should be returned in `bands` in a list like `[\"B02\", \"B03\", \"B04\", \"B12\", \"VV\"]`. \n",
    "Only those bands will then be read and returned in a dict. Restricting this to only S1 or S2 band names reduces the reading accesses needed and increases throughput.\n",
    "\n",
    "The data is returned as `Tuple[Dict[str, numpy.nd_array], List[str]]`. \n",
    "There is no resizing or stacking performed on the bands. If you want to use the DS for pytorch training inside a DataLoader the values have to be returned as tensors with some dimension restrictions. \n",
    "To account for that, the DS accepts two parameters `process_bands_fn` and `process_labels_fn`, each being functions that accept the repective elements of the tuple mentioned before. \n",
    "`process_bands_fn` also takes in as a second parameter the name of the bands. \n",
    "If you want to simply stack the bands and interpolate to an equal size, there is a function provided in `BENv2TorchUtils` where you can select the interpolation mode via `functools.partial`.\n",
    "To convert the `List[str]` of the labels to a multi-hot tensor, a function is provided in `BENv2TorchUtils` as well.\n",
    "\n",
    "To select which images to include in the DS, a list of `keys` can be passed. \n",
    "If `None` is passed (default), all S2 keys in the LMDB data base will be used.\n",
    "Here we will use the patch names that we read earlier.\n",
    "\n",
    "Augmentations can be passed as `transforms`. \n",
    "They have to accept whatever is returned from the `process_bands_fn` function if set or the dictionary that is returned by default.\n",
    "\n",
    "If we need the patch name that was read - e.g. for visualization purposes - we can set `return_patchname` to `True`. \n",
    "Instead of `(Image, Label)` the DS will then return `(Image, Label, Patch name)`.\n",
    "\n",
    "If you want additional information what the DataSet is doing, you can pass `verbose=True` to print different status updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34238ee4-de4e-4465-ab19-66678a9fd3cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T13:00:12.493845396Z",
     "start_time": "2024-01-19T13:00:04.335594295Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from BENv2DataSet import BENv2DataSet\n",
    "from BENv2Stats import means, stds\n",
    "from BENv2TorchUtils import ben_19_labels_to_multi_hot\n",
    "from BENv2TorchUtils import stack_and_interpolate\n",
    "\n",
    "image_size = 120\n",
    "upsample_mode = \"nearest\"\n",
    "\n",
    "bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B05\", \"B06\", \"B07\", \"B11\", \"B12\", \"B8A\", \"VV\", \"VH\"]\n",
    "\n",
    "# there are different combinations depending on the mode available, select the right mode\n",
    "mean = means[f\"{image_size}_{upsample_mode}\"]\n",
    "std = stds[f\"{image_size}_{upsample_mode}\"]\n",
    "# only select the right bands\n",
    "mean = [mean[b] for b in bands]\n",
    "std = [std[b] for b in bands]\n",
    "\n",
    "# some default transformations that we can pass to the data set class\n",
    "transforms = {\n",
    "    \"train\": torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomVerticalFlip(),\n",
    "            torchvision.transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    ),\n",
    "    \"validation\": torchvision.transforms.Compose([torchvision.transforms.Normalize(mean, std)]),\n",
    "    \"test\": torchvision.transforms.Compose([torchvision.transforms.Normalize(mean, std)]),\n",
    "}\n",
    "\n",
    "# create a DataSet for each split\n",
    "ds = {\n",
    "    s: BENv2DataSet(\n",
    "        image_lmdb_file=path_image_lmdb,\n",
    "        label_file=path_labels,\n",
    "        s2s1_mapping_file=path_s2v2name_to_s1v1name,\n",
    "        bands=bands,\n",
    "        process_bands_fn=partial(\n",
    "            stack_and_interpolate, img_size=image_size, upsample_mode=upsample_mode\n",
    "        ),\n",
    "        process_labels_fn=ben_19_labels_to_multi_hot,\n",
    "        transforms=transforms[s],\n",
    "        keys=patches[s],\n",
    "        return_patchname=False,\n",
    "    )\n",
    "    for s in patches\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6914f0-5cdc-403e-ae89-c592002aea6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T13:05:13.012556443Z",
     "start_time": "2024-01-19T13:00:12.527444648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch progress:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c4653a3229f489e965edce138a61c5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training:   0%|          | 0/1065 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a07ca3191f04adbbc29fdf84a8988b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "validating:   0%|          | 0/546 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2be865f71a8400bade2714ac9cb3ea7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dl[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m], position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dl[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m\"\u001B[39m], position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidating\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dl[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m], position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/site-packages/tqdm/notebook.py:249\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    248\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1329\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1291\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1292\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1295\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1296\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1297\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1122\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1133\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1136\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1138\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    928\u001B[0m     deadline \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic() \u001B[38;5;241m+\u001B[39m timeout\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n\u001B[1;32m    933\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [key\u001B[38;5;241m.\u001B[39mfileobj \u001B[38;5;28;01mfor\u001B[39;00m (key, events) \u001B[38;5;129;01min\u001B[39;00m ready]\n",
      "File \u001B[0;32m~/micromamba/envs/benv2-igarss-v2/lib/python3.9/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    414\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "bs = 256\n",
    "workers = 8\n",
    "\n",
    "# create a DataLoader for each DS\n",
    "dl = {\n",
    "    \"train\": DataLoader(\n",
    "        ds[\"train\"], batch_size=bs, num_workers=workers, pin_memory=False, shuffle=True\n",
    "    ),\n",
    "    \"validation\": DataLoader(\n",
    "        ds[\"validation\"],\n",
    "        batch_size=bs,\n",
    "        num_workers=workers,\n",
    "        pin_memory=False,\n",
    "        shuffle=False,\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        ds[\"test\"], batch_size=bs, num_workers=workers, pin_memory=False, shuffle=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# create a fake train-val-loop\n",
    "epochs = 5\n",
    "for e in tqdm(range(epochs), position=0, desc=\"Epoch progress\"):\n",
    "    for batch in tqdm(dl[\"train\"], position=1, leave=False, desc=\"training\"):\n",
    "        pass\n",
    "\n",
    "    for batch in tqdm(dl[\"validation\"], position=1, leave=False, desc=\"validating\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "for batch in tqdm(dl[\"test\"], position=1, leave=False, desc=\"testing\"):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224472a3-f9f9-4609-a9de-0b619dc9b72f",
   "metadata": {},
   "source": [
    "# Data Module\n",
    "\n",
    "The Data Module is a wrapper around the DataSet and DataLoader that is used by pytorch lightning. It also handles the splitting of the data into train, validation and test set according to the split file as shown above. The augmentation is also handled by the Data Module in the same way as shown above if no augmentation is passed to the Data Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17d6a1-e841-4f47-99b1-3ce7a45e5965",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-19T13:05:13.016822986Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from BENv2DataModule import BENv2DataModule\n",
    "\n",
    "total = 64\n",
    "\n",
    "bs = 256\n",
    "workers = 8\n",
    "\n",
    "dm = BENv2DataModule(\n",
    "    image_lmdb_file=path_image_lmdb,\n",
    "    label_file=path_labels,\n",
    "    s2s1_mapping_file=path_s2v2name_to_s1v1name,\n",
    "    split_file=path_split,\n",
    "    batch_size=bs,\n",
    "    num_workers=workers\n",
    ")\n",
    "\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "t0 = time()\n",
    "for j in range(2):\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), total=len(dm.train_dataloader())):\n",
    "        pass\n",
    "\n",
    "t1 = time()\n",
    "print(f\"{bs:4d} {workers:2d} --  Took {t1-t0}s for {(i+1)*bs*(j+1)} samples\")\n",
    "print(f\"            {(i+1)*bs*(j+1) / (t1-t0)} samples/s\")\n",
    "print(f\"            {(t1-t0)/((i+1)*bs*(j+1))} s/sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887e4dd-9c04-4806-bf5d-5046284f0649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
